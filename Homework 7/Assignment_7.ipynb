{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make-up homework assignment\n",
    "\n",
    "\n",
    "## **Important**: rules for handing in the make-up assignment:\n",
    "\n",
    "- You have to make this homework assignment on your own, so not in a team!\n",
    "- Hand in the notebook (and nothing else) named as follows: StudentName_snumber.ipynb\n",
    "- Provide clear and complete answers to the questions below under a separate header (not hidden somewhere in your source code), and make sure to explain your answers / motivate your choices. Add Markdown cells if necessary.\n",
    "- Source code, output graphs, derivations, etc., should be included in the notebook. Make sure it is clear what each plot represents. \n",
    "- Hand-in: upload to Blackboard.\n",
    "- For problems or questions (that are not readily answered by the lecture slides, book and/or a quick Google search) please send an email to tomh@cs.ru.nl.\n",
    "\n",
    "\n",
    "# Part 1: classification and clustering\n",
    "\n",
    "In this exercise we will apply Nearest Neighbor and K-means clustering to the seven included data sets `synth1` to `synth7`. These each consist of an attribute matrix `X` containing two attributes and a class vector `y`, and are split into a training set and test set. Note that in every subquestion, you should consider all data sets.\n",
    "\n",
    "1.1 Create a scatter plot of X (both training and test set) for each of the data sets, with points colored according to their class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe for each of the data sets the patterns that you observe in the data. How are the attributes of the data points related to their class?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Double click to write your answers to part 1.1 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Use K-nearest neighbors classification with Euclidean distance measure to classify the test data. First, use ten-fold cross-validation on the training data (`X_train` / `y_train`) to determine the optimal K (vary K from 1 to 25). Plot the mean classification error rate across the ten folds as a function of K and determine for which value the classification error is lowest. With K set to this value, now classify the test data (`X_test`) using the training data as input and compute the classification error. Make a new scatter plot of the data, with points in the training data colored according to their actual class and points in the test data according to their predicted class; use different plot symbols (e.g., circles and squares) for the training and test data. Do the above for all data sets `synth1` to `synth7`.\n",
    "\n",
    "   + You can use the `KNeighborsClassifier()` function from the package `sklearn.neighbors` to perform the nearest neighbor classification. Classify the data points in the test data based on its nearest neighbors in the training data.\n",
    "   + You can use use the `Kfold()` function from the package `sklearn.model_selection` for the cross-validation. You should only use the training data for this part: in other words, the training data will itself be split into smaller training and test subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#part 1.2 code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the data sets, describe how well the test data is classified, and what patterns (if any) there are in the mis-classified data points and how those patterns might be explained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Double click to write your answers to part 1.2 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Use K-means clustering with Euclidean distance measure to cluster the data in X. First, run the K-means clustering for K = 1, …, 10 clusters and compute the purity and entropy. Plot purity and entropy as a function of K. Again, do this for all data sets `synth1` to `synth7`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#part 1.3 code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the optimal number of clusters for each of the data sets? Does this correspond to the actual number of clusters in the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Double click to write your answers to part 1.3 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4 Make a new scatter plot of the data for every data set, with points colored to correspond to the K-means clustering (with K set to the actual number of clusters in the data).\n",
    "\n",
    "   + You can use the function `k_means()` from the package sklearn.cluster to perform the clustering. The `clusterVal` function (in the `clusterVal.py` file) can be used to obtain the purity and entropy.\n",
    "   + When comparing the K-means clustering to the real clustering, keep in mind that the numbers assigned to clusters by the `k_means()` function are arbitrary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#part 1.4 code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this to the scatter plots made in 1. How well does the K-means clustering correspond to the actual clustering in the data? To the extent that they do not correspond, can you explain where and why the K-means clustering goes wrong?\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Double click to write your answers to part 1.4 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Ensemble methods\n",
    "\n",
    "In this exercise we will use ensemble methods in combination with Nearest Neighbor classification. In particular, we will use a method called ‘bagging’. Bagging works by creating M new training data sets by randomly sampling (with replacement) N data points from the original training data set. The test data is then classified with (in this case) the Nearest Neighbor classifier using each of these new training data sets, which results in M classifications for each data point in the test data. The final classification for each data point is the class that occurs most often among those M classifications.\n",
    "\n",
    "For this part of the exercise, we will use the `synth4` data set. We will not use any cross-validation, but rather use the division into training and test data already in the data file. Use K = 1 and Euclidean distance (unless otherwise specified).\n",
    "\n",
    "2.1. Explain in your own words how ensemble methods in general and bagging in particular can improve classification compared to just running the Nearest Neighbor algorithm once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Double click to write your answers to part 2.1 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Classify the test data with the KNN classifier and compute the classification error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#part 2.2 code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Use Nearest Neighbor classification with bagging to classify the test data. Use M = 1000 ‘bags’, and set N to 10% of the training data sample size. Compute the classification error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#part 2.3 code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did the classification improve compared to the classification without bagging in step 2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Double click to write your answers to part 2.3 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Make a scatter plot of the data. Color the training data points according to their class. For the test data points, use different colors/plot symbols/points sizes to indicate which of those are A) correctly classified both times (step 2 and 3), B) correctly classified in step 3 but not step 2 and C) not correctly classified in step 3. Make sure that they are visually easy to distinguish from each other and the training data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#part 2.4 code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see any pattern in the test data points, where the bagging improves the classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Double click to write your answers to part 2.4 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Repeat step 3 but set N to 5%, 50% and 100% of the training data sample size, and compute the classification error for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#part 2.5 code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain why the bagging classifier works better when you do not use all of the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Double click to write your answers to part 2.5 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Repeat steps 2 and 3 but with K set to the optimal value you determined for `synth4` in part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#part 2.6 code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the bagging still improve the classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Double click to write your answers to part 2.6 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Suppose you have a very small data set with one test data point of class A and five training data points, with the closest one of class B and the other four of class A. When bagging, what is the proportion of bags for which the test data point would be classified as class B (that is, what is the probability that the ‘class B’ point is selected when sampling with replacement)? Answer this question for N = 1, N = 2, N = 5 and N = 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Double click to write your answers to part 2.7 here"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
